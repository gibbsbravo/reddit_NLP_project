{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AGB\\Desktop\\WeCloud_Materials\\Project\\Subreddit_Comments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/AGB/Desktop/WeCloud_Materials/Project/Subreddit_Comments')\n",
    "print(os.getcwd())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Libertarian            0.473725\n",
       "LateStageCapitalism    0.239021\n",
       "Conservative           0.228165\n",
       "Anarchism              0.059089\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load processed comments from desired subreddits \n",
    "subreddits = ['Anarchism','Conservative','LateStageCapitalism','Libertarian']\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "for i in subreddits: \n",
    "    tempdf = pd.read_csv(i+'_processed_comments.csv')\n",
    "    tempdf['subreddit'] = i\n",
    "    df = pd.concat([df,tempdf])\n",
    "\n",
    "df = df.sample(frac=0.3, random_state=10)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df['subreddit'].value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally aggregate subreddits into conservative / liberal in order to highlight terms which differ\n",
    "political_map = {'Libertarian':'Conservative','Conservative':'Conservative',\n",
    "                'LateStageCapitalism':'Liberal','Anarchism':'Liberal'}\n",
    "\n",
    "df['subreddit'] = df['subreddit'].map(political_map) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from ast import literal_eval\n",
    "\n",
    "# Label encode target subreddit / assigned political orientation\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "df['subreddit'] = le.fit_transform(df['subreddit'])\n",
    "\n",
    "tokens = [] \n",
    "\n",
    "for i in range(len(df['tokens'])):\n",
    "    tokens.append(literal_eval(df['tokens'][i]))\n",
    "\n",
    "df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train / test sets \n",
    "y = df['subreddit']\n",
    "X = df['new_comment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLP packages and set stop words \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english')+list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text further to break into tokens\n",
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer() #lemma = wordnet_lemmatizer.lemmatize(i)\n",
    "porter = nltk.PorterStemmer() #lemma = porter.stem(i)\n",
    "snowball = nltk.SnowballStemmer('english') #lemma = snowball.stem(i)\n",
    "\n",
    "add_stop = [\"'s\",\"n't\",\"''\",\"'m\",'http','.com',\"--\",\"gt\"]\n",
    "\n",
    "def NLTKprocess(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_list = []\n",
    "    for i in tokens:\n",
    "        if(i in stop_words or re.search(\"^[a-zA-Z0-9\\-']*$\", i) is None):\n",
    "            continue\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(i)\n",
    "            if lemma not in add_stop:\n",
    "                stemmed_list.append(lemma)\n",
    "    return stemmed_list\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=NLTKprocess)\n",
    "# Default w/o lemmatizing: \n",
    "#vectorizer = TfidfVectorizer(stop_words = stop_words,tokenizer=word_tokenize)\n",
    "\n",
    "# Learn and transform train documents\n",
    "vectorised_train_documents = vectorizer.fit_transform(X_train)\n",
    "vectorised_test_documents = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Use LinearSVC as classifier\n",
    "classifier = LinearSVC(C=1)\n",
    "classifier.fit(vectorised_train_documents, y_train)\n",
    "\n",
    "SVM_train_predictions = classifier.predict(vectorised_train_documents)\n",
    "SVM_test_predictions = classifier.predict(vectorised_test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: hijab kennedy assimilate pakistan bombshell dems brigade conceal conservative unborn hollywood reparation pelosi amnesty bannon\n",
      "1: wic imperialism rehabilitation thriving anarchism revolutionary bourgeoisie 1937 imperialist solidarity boa bourgeois lobbying fash praxis\n",
      "2: ultra-right populace discord cuck che statist threatened houston censor tariff lp nap nigger libertarianism libertarian\n"
     ]
    }
   ],
   "source": [
    "# Define functions to output key terms for subreddit\n",
    "\n",
    "def print_top10(vectorizer, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-15:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \" \".join(feature_names[j] for j in top10)))\n",
    "\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "\n",
    "if len(np.sort(y_train.unique())) <= 2:\n",
    "    show_most_informative_features(vectorizer, classifier, n=20)\n",
    "else:\n",
    "    print_top10(vectorizer, classifier, np.sort(y_train.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The main metrics for Text Classification are:\n",
    "\n",
    "**Precision**: Number of documents correctly assigned to a category out of the total number of documents predicted.  \n",
    "**Recall**: Number of documents correctly assigned to a category out of the total number of documents in such category.  \n",
    "**F1**: Metric that combines precision and recall using the harmonic mean.  \n",
    "\n",
    "If the evaluation is being done in multi-class or multi-label environments, the method becomes slightly more complicated because the quality metrics have to be either shown per category, or globally aggregated. There are two main aggregation approaches:  \n",
    "\n",
    "**Micro-average**: Every assignment (document, label) has the same importance. Common categories have more effect over the aggregate quality than smaller ones.  \n",
    "**Macro-average**: The quality for each category is calculated independently and their average is reported. All the categories are equally important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Accuracy: 0.6115, Precision: 0.6115, Recall: 0.6115, F1-measure: 0.6115\n",
      "\n",
      "Macro-average quality numbers\n",
      "Accuracy: 0.6115, Precision: 0.5929, Recall: 0.5753, F1-measure: 0.5810\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    " \n",
    "test_predictions = SVM_test_predictions\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "precision = precision_score(y_test, test_predictions, average='micro')\n",
    "recall = recall_score(y_test, test_predictions, average='micro')\n",
    "f1 = f1_score(y_test, test_predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(accuracy,precision, recall, f1))\n",
    " \n",
    "precision = precision_score(y_test, test_predictions, average='macro')\n",
    "recall = recall_score(y_test, test_predictions,average='macro')\n",
    "f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "\n",
    "print()\n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(accuracy,precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.783309\n",
       "0    0.216691\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "counter = []\n",
    "\n",
    "for i in range(len(test_predictions)):\n",
    "    counter.append((test_predictions[i]==y_test.values[i]).sum())\n",
    "\n",
    "counter = pd.DataFrame(counter)\n",
    "counter[0].value_counts()/len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Logistic Regression Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23774</td>\n",
       "      <td>1764</td>\n",
       "      <td>25538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6101</td>\n",
       "      <td>4657</td>\n",
       "      <td>10758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>29875</td>\n",
       "      <td>6421</td>\n",
       "      <td>36296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted      0     1    All\n",
       "True                         \n",
       "0          23774  1764  25538\n",
       "1           6101  4657  10758\n",
       "All        29875  6421  36296"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Combined SVM Confusion Matrix:')\n",
    "pd.crosstab(y_test, test_predictions, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conservative' 'Liberal']\n"
     ]
    }
   ],
   "source": [
    "print(le.inverse_transform(np.sort(y_train.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciKit Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can instead use multinomial NB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(vectorised_train_documents, y_train)\n",
    "\n",
    "NB_train_predictions = clf.predict(vectorised_train_documents)\n",
    "NB_test_predictions = clf.predict(vectorised_test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Accuracy: 0.5453, Precision: 0.5453, Recall: 0.5453, F1-measure: 0.5453\n",
      "\n",
      "Macro-average quality numbers\n",
      "Accuracy: 0.5453, Precision: 0.6917, Recall: 0.3401, F1-measure: 0.3254\n"
     ]
    }
   ],
   "source": [
    "test_predictions = NB_test_predictions\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "precision = precision_score(y_test, test_predictions, average='micro')\n",
    "recall = recall_score(y_test, test_predictions, average='micro')\n",
    "f1 = f1_score(y_test, test_predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(accuracy,precision, recall, f1))\n",
    " \n",
    "precision = precision_score(y_test, test_predictions, average='macro')\n",
    "recall = recall_score(y_test, test_predictions,average='macro')\n",
    "f1 = f1_score(y_test, test_predictions, average='macro')\n",
    "\n",
    "print()\n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(accuracy,precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Logistic Regression Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "      <td>190</td>\n",
       "      <td>1984</td>\n",
       "      <td>2236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1603</td>\n",
       "      <td>188</td>\n",
       "      <td>6531</td>\n",
       "      <td>8322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>1723</td>\n",
       "      <td>6763</td>\n",
       "      <td>8674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>337</td>\n",
       "      <td>270</td>\n",
       "      <td>16455</td>\n",
       "      <td>17064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>14</td>\n",
       "      <td>2178</td>\n",
       "      <td>2371</td>\n",
       "      <td>31733</td>\n",
       "      <td>36296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0     1     2      3    All\n",
       "True                                   \n",
       "0          11    51   190   1984   2236\n",
       "1           0  1603   188   6531   8322\n",
       "2           1   187  1723   6763   8674\n",
       "3           2   337   270  16455  17064\n",
       "All        14  2178  2371  31733  36296"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Combined Logistic Regression Confusion Matrix:')\n",
    "pd.crosstab(y_test, test_predictions, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Stacking and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_out = df.iloc[X_train.index].copy()\n",
    "X_test_out = df.iloc[X_test.index].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_out.drop(['author','comment','created_utc','id','link_id','parent_id','new_comment','tokens','subreddit'], axis=1, inplace=True)\n",
    "X_test_out.drop(['author','comment','created_utc','id','link_id','parent_id','new_comment','tokens','subreddit'], axis=1, inplace=True)\n",
    "\n",
    "X_train_out.drop(['year','month'], axis=1, inplace=True)\n",
    "X_test_out.drop(['year','month'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_out['SVM_pred'] = SVM_train_predictions\n",
    "\n",
    "dummy_CODE = X_train_out['SVM_pred']\n",
    "dummy_CODE = pd.get_dummies(dummy_CODE)\n",
    "dummy_CODE = dummy_CODE.add_suffix('_SVM')\n",
    "X_train_out = X_train_out.join(dummy_CODE,how='outer')\n",
    "X_train_out.drop(['SVM_pred',], axis=1, inplace=True)\n",
    "\n",
    "X_test_out['SVM_pred'] = SVM_test_predictions\n",
    "\n",
    "dummy_CODE = X_test_out['SVM_pred']\n",
    "dummy_CODE = pd.get_dummies(dummy_CODE)\n",
    "dummy_CODE = dummy_CODE.add_suffix('_SVM')\n",
    "X_test_out = X_test_out.join(dummy_CODE,how='outer')\n",
    "X_test_out.drop(['SVM_pred',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_out['NB_pred'] = NB_train_predictions\n",
    "\n",
    "dummy_CODE = X_train_out['NB_pred']\n",
    "dummy_CODE = pd.get_dummies(dummy_CODE)\n",
    "dummy_CODE = dummy_CODE.add_suffix('_NB')\n",
    "X_train_out = X_train_out.join(dummy_CODE,how='outer')\n",
    "X_train_out.drop(['NB_pred',], axis=1, inplace=True)\n",
    "\n",
    "X_test_out['NB_pred'] = NB_test_predictions\n",
    "\n",
    "dummy_CODE = X_test_out['NB_pred']\n",
    "dummy_CODE = pd.get_dummies(dummy_CODE)\n",
    "dummy_CODE = dummy_CODE.add_suffix('_NB')\n",
    "X_test_out = X_test_out.join(dummy_CODE,how='outer')\n",
    "X_test_out.drop(['NB_pred',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3_SVM</th>\n",
       "      <td>0.079716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_SVM</th>\n",
       "      <td>0.064136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.063476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>0.062385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gunning_fog</th>\n",
       "      <td>0.058288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>0.055401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flesch</th>\n",
       "      <td>0.055156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_SVM</th>\n",
       "      <td>0.055051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>0.054562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_syllables</th>\n",
       "      <td>0.054035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blob_polarity</th>\n",
       "      <td>0.051279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v_compound</th>\n",
       "      <td>0.051274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blob_subj</th>\n",
       "      <td>0.050737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v_neutral</th>\n",
       "      <td>0.045428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difficult_words</th>\n",
       "      <td>0.043995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v_positive</th>\n",
       "      <td>0.036815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v_negative</th>\n",
       "      <td>0.033665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_NB</th>\n",
       "      <td>0.024709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_NB</th>\n",
       "      <td>0.022887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_NB</th>\n",
       "      <td>0.021062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_SVM</th>\n",
       "      <td>0.015547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_NB</th>\n",
       "      <td>0.000396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 importance\n",
       "3_SVM              0.079716\n",
       "2_SVM              0.064136\n",
       "score              0.063476\n",
       "day                0.062385\n",
       "gunning_fog        0.058288\n",
       "hour               0.055401\n",
       "flesch             0.055156\n",
       "1_SVM              0.055051\n",
       "word_count         0.054562\n",
       "avg_syllables      0.054035\n",
       "blob_polarity      0.051279\n",
       "v_compound         0.051274\n",
       "blob_subj          0.050737\n",
       "v_neutral          0.045428\n",
       "difficult_words    0.043995\n",
       "v_positive         0.036815\n",
       "v_negative         0.033665\n",
       "2_NB               0.024709\n",
       "1_NB               0.022887\n",
       "3_NB               0.021062\n",
       "0_SVM              0.015547\n",
       "0_NB               0.000396"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "        n_estimators = 100, # number of trees in the forest, default is 10\n",
    "        max_features = 'sqrt', # number of features to be split on each node\n",
    "        n_jobs = 4, # the number of jobs is set to the number of cores\n",
    "        random_state = 7)\n",
    "    \n",
    "rf_clf.fit(X_train_out, y_train)\n",
    "\n",
    "# check the variable importance\n",
    "importance = rf_clf.feature_importances_\n",
    "importance = pd.DataFrame(importance, \n",
    "                          columns=[\"importance\"],\n",
    "                          index = X_train_out.columns\n",
    "                         )\n",
    "importance.sort_values(by='importance',ascending=False,inplace=True)\n",
    "\n",
    "importance[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBoost classifier on training set: 0.694\n",
      "Accuracy of XGBoost classifier on test set: 0.623\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=150, learning_rate=0.05).fit(X_train_out, y_train.ravel())\n",
    "gbm_predictedValue = gbm.predict_proba(X_test_out)\n",
    "\n",
    "y_pred = gbm_predictedValue[:,1]\n",
    "gbm_train_pred = gbm.predict(X_train_out)\n",
    "gbm_test_pred = gbm.predict(X_test_out)\n",
    "\n",
    "#print('AUC using XGBoost is {:.4f}'.format(roc_auc_score(y_test_out, y_pred)))\n",
    "print('Accuracy of XGBoost classifier on training set: {:.3f}'\n",
    "     .format(gbm.score(X_train_out, y_train)))\n",
    "print('Accuracy of XGBoost classifier on test set: {:.3f}'\n",
    "     .format(gbm.score(X_test_out, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest classifier on training set: 0.693\n",
      "Accuracy of Random Forest classifier on test set: 0.621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Random forest model\n",
    "rf_clf = RandomForestClassifier(\n",
    "        n_estimators = 100, # number of trees in the forest, default is 10\n",
    "        max_depth = 7, #max depth of each tree\n",
    "        max_features = 'sqrt', # number of features to be split on each node\n",
    "        n_jobs = 4, # the number of jobs is set to the number of cores\n",
    "        random_state = 7)\n",
    "\n",
    "rf_clf.fit(X_train_out, y_train.ravel())\n",
    "rf_clf_predictedValue = rf_clf.predict_proba(X_test_out)\n",
    "\n",
    "y_pred = rf_clf_predictedValue[:,1]\n",
    "rf_train_pred = rf_clf.predict(X_train_out)\n",
    "rf_test_pred = rf_clf.predict(X_test_out)\n",
    "\n",
    "#print('AUC using Random Forest is {:.4f}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('Accuracy of Random Forest classifier on training set: {:.3f}'\n",
    "     .format(rf_clf.score(X_train_out, y_train)))\n",
    "print('Accuracy of Random Forest classifier on test set: {:.3f}'\n",
    "     .format(rf_clf.score(X_test_out, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.692\n",
      "Accuracy of Logistic Regression classifier on test set: 0.621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logr_clf = LogisticRegression(C=1).fit(X_train_out, y_train)\n",
    "logr_predictedValue = logr_clf.predict_proba(X_test_out)\n",
    "\n",
    "y_pred = logr_predictedValue[:,1]\n",
    "logr_train_pred = logr_clf.predict(X_train_out)\n",
    "logr_test_pred = logr_clf.predict(X_test_out)\n",
    "\n",
    "#print('AUC using Logistic Regression is {:.4f}'.format(roc_auc_score(y_test, y_pred)))\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.3f}'\n",
    "     .format(logr_clf.score(X_train_out, y_train)))\n",
    "print('Accuracy of Logistic Regression classifier on test set: {:.3f}'\n",
    "     .format(logr_clf.score(X_test_out, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Bayesian Classifier - Returns Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "documents = [(list(df['tokens'].iloc[i]), df['subreddit'].iloc[i])\n",
    "             for i in range(len(df['tokens']))]\n",
    "\n",
    "flatten = [item for sublist in df['tokens'] for item in sublist]\n",
    "all_words = nltk.FreqDist(flatten)\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set that we'll train our classifier with\n",
    "test_cutoff = int(np.around(.8 * (len(featuresets)),0))\n",
    "\n",
    "training_set = featuresets[:test_cutoff]\n",
    "\n",
    "# set that we'll test against.\n",
    "testing_set = featuresets[test_cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier training accuracy percent: 53.4600725536116\n",
      "Classifier test accuracy percent: 50.61989163375884\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "#print(\"Classifier training accuracy percent:\",(nltk.classify.accuracy(classifier, training_set))*100)\n",
    "print(\"Classifier test accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anarchism' 'Conservative' 'LateStageCapitalism' 'Libertarian']\n",
      "Most Informative Features\n",
      "                 anarchy = True                0 : 1      =     94.3 : 1.0\n",
      "                 manning = True                0 : 3      =     72.9 : 1.0\n",
      "                comrades = True                0 : 3      =     63.2 : 1.0\n",
      "                 mueller = True                1 : 2      =     56.9 : 1.0\n",
      "               anarchist = True                0 : 2      =     54.3 : 1.0\n",
      "              indigenous = True                0 : 3      =     51.3 : 1.0\n",
      "                 chelsea = True                0 : 3      =     51.3 : 1.0\n",
      "          libertarianism = True                3 : 2      =     49.1 : 1.0\n",
      "                     dsa = True                0 : 3      =     45.9 : 1.0\n",
      "                  ancaps = True                0 : 1      =     42.6 : 1.0\n",
      "             imperialist = True                0 : 3      =     38.2 : 1.0\n",
      "             imperialism = True                0 : 1      =     37.5 : 1.0\n",
      "               stateless = True                0 : 1      =     34.9 : 1.0\n",
      "                    memo = True                1 : 2      =     32.0 : 1.0\n",
      "               voluntary = True                3 : 1      =     29.8 : 1.0\n",
      "                 turkish = True                0 : 3      =     29.7 : 1.0\n",
      "               classless = True                0 : 3      =     29.7 : 1.0\n",
      "                   solar = True                3 : 1      =     29.6 : 1.0\n",
      "            libertarians = True                3 : 0      =     29.1 : 1.0\n",
      "               violation = True                3 : 2      =     27.5 : 1.0\n",
      "             reparations = True                1 : 3      =     27.2 : 1.0\n",
      "                sessions = True                3 : 2      =     25.7 : 1.0\n",
      "                 elitist = True                0 : 3      =     24.3 : 1.0\n",
      "                 stirner = True                0 : 3      =     24.3 : 1.0\n",
      "                shutdown = True                1 : 2      =     24.1 : 1.0\n",
      "            exploitation = True                0 : 1      =     23.8 : 1.0\n",
      "                    daca = True                1 : 0      =     23.0 : 1.0\n",
      "              capitalist = True                2 : 1      =     22.6 : 1.0\n",
      "                    dems = True                1 : 0      =     22.0 : 1.0\n",
      "                organize = True                0 : 1      =     21.6 : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AGB\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print(le.inverse_transform(np.sort(y_train.unique())))\n",
    "inform_features = classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Analysis - Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(24183 unique tokens: ['bc', 'dumb', 'forgot', 'idiot', 'im']...)\n"
     ]
    }
   ],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "subreddit = 0\n",
    "\n",
    "sub_df = df[df['subreddit']== subreddit]\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index. \n",
    "\n",
    "dictionary = corpora.Dictionary(sub_df['tokens'])\n",
    "dictionary.save('dictionary.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11086\n",
      "[(79, 1), (321, 1), (1086, 1), (1087, 1), (1088, 1), (1089, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in sub_df['tokens']]\n",
    "corpora.MmCorpus.serialize('corpus.mm', doc_term_matrix)\n",
    "\n",
    "print(len(doc_term_matrix))\n",
    "print(doc_term_matrix[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used: 127.05s\n"
     ]
    }
   ],
   "source": [
    "from time import time \n",
    "\n",
    "start = time()\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50)\n",
    "print('used: {:.2f}s'.format(time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.save('topic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, '0.031*\"amp\" + 0.017*\"--\" + 0.008*\"animals\" + 0.008*\"eat\"'), (4, '0.018*\"people\" + 0.010*\"like\" + 0.009*\"think\" + 0.007*\"get\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "loading = LdaModel.load('topic.model')\n",
    "print(loading.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pyLDAvis visualization\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "d = gensim.corpora.Dictionary.load('dictionary.dict')\n",
    "c = gensim.corpora.MmCorpus('corpus.mm')\n",
    "lda = gensim.models.LdaModel.load('topic.model')\n",
    "\n",
    "data = pyLDAvis.gensim.prepare(lda, c, d)\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
