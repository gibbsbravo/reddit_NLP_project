{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AGB\\Desktop\\WeCloud_Materials\\Project\\Subreddit_Comments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop = set(stopwords.words('english')+list(string.punctuation))\n",
    "\n",
    "os.chdir('C:/Users/AGB/Desktop/WeCloud_Materials/Project/Subreddit_Comments')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read comments dataframe \n",
    "subreddit = 'Anarchism'\n",
    "\n",
    "comment_title = subreddit+'_comments.csv'\n",
    "comments = pd.read_csv(comment_title,\n",
    "                          dtype = {'author':object,\n",
    "                                   'body':str,\n",
    "                                   'score':float,\n",
    "                                   'created_utc':object,\n",
    "                                   'id':object,\n",
    "                                   'link_id':object,\n",
    "                                   'parent_id':object,\n",
    "                                   'hour':float,\n",
    "                                   'day':float,\n",
    "                                   'month':float,\n",
    "                                   'year':float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows without values in year or comment body\n",
    "comments.dropna(subset = ['year','body'],inplace=True, axis=0)\n",
    "comments['created_utc'] = comments['created_utc'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns and copy dataframe for easier processing\n",
    "df = comments.copy()\n",
    "df.rename(columns={'body': 'comment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11603 entries, 0 to 11602\n",
      "Data columns (total 11 columns):\n",
      "author         11603 non-null object\n",
      "comment        11603 non-null object\n",
      "score          11603 non-null float64\n",
      "created_utc    11603 non-null float64\n",
      "id             11603 non-null object\n",
      "link_id        11603 non-null object\n",
      "parent_id      11603 non-null object\n",
      "hour           11603 non-null float64\n",
      "day            11603 non-null float64\n",
      "month          11603 non-null float64\n",
      "year           11603 non-null float64\n",
      "dtypes: float64(6), object(5)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>I just realised you are right and that im an i...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>ds0n5o3</td>\n",
       "      <td>t3_7mxfem</td>\n",
       "      <td>t1_ds0mznv</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jphuffinstuff</td>\n",
       "      <td>No problem! It happens to the best of us!</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>ds0ncnp</td>\n",
       "      <td>t3_7mxfem</td>\n",
       "      <td>t1_ds0n5o3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thrashgoat555</td>\n",
       "      <td>What missing information would that be.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>ds0nfc8</td>\n",
       "      <td>t3_7n88zr</td>\n",
       "      <td>t1_ds0ki36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adventure_Inc</td>\n",
       "      <td>As someone who lives in Chicago, this pisses m...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>ds0nfj5</td>\n",
       "      <td>t3_7nbjsc</td>\n",
       "      <td>t3_7nbjsc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Happy new year btw :)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>ds0ng9h</td>\n",
       "      <td>t3_7mxfem</td>\n",
       "      <td>t1_ds0ncnp</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                                            comment  score  \\\n",
       "0      [deleted]  I just realised you are right and that im an i...    2.0   \n",
       "1  jphuffinstuff          No problem! It happens to the best of us!    1.0   \n",
       "2  thrashgoat555            What missing information would that be.    3.0   \n",
       "3  Adventure_Inc  As someone who lives in Chicago, this pisses m...   17.0   \n",
       "4      [deleted]                             Happy new year btw :)     1.0   \n",
       "\n",
       "    created_utc       id    link_id   parent_id  hour  day  month    year  \n",
       "0  1.514765e+09  ds0n5o3  t3_7mxfem  t1_ds0mznv   0.0  1.0    1.0  2018.0  \n",
       "1  1.514765e+09  ds0ncnp  t3_7mxfem  t1_ds0n5o3   0.0  1.0    1.0  2018.0  \n",
       "2  1.514765e+09  ds0nfc8  t3_7n88zr  t1_ds0ki36   0.0  1.0    1.0  2018.0  \n",
       "3  1.514765e+09  ds0nfj5  t3_7nbjsc   t3_7nbjsc   0.0  1.0    1.0  2018.0  \n",
       "4  1.514765e+09  ds0ng9h  t3_7mxfem  t1_ds0ncnp   0.0  1.0    1.0  2018.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining comments: 11599\n"
     ]
    }
   ],
   "source": [
    "# Replace author NaNs and drop [removed] comments\n",
    "df['author'].fillna('None',inplace=True)\n",
    "df = df[df['comment']!='[removed]']\n",
    "df = df[df['comment'].apply(lambda x: str(x).isdigit() == False)]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "print('Remaining comments:',len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to ensure there are no errors\n",
    "counta = 0\n",
    "countb = 0\n",
    "\n",
    "for i in df['comment']:\n",
    "    if isinstance(i,float):\n",
    "        counta+=1\n",
    "    elif i.isdigit() == True:\n",
    "        countb+=1\n",
    "        \n",
    "print(counta)\n",
    "countb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to process the comments\n",
    "def processText(text):\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Convert www.* or https?://* to URL - removed as may eliminate information\n",
    "    #text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',text)\n",
    "    \n",
    "    # Remove additional white spaces\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    \n",
    "    # Replace #word with word\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "      \n",
    "    # Trim\n",
    "    text = text.strip('\\'\"')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def replaceTwoOrMore(s):\n",
    "    # Look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt at a spellchecker using NLTK and textblob although runs EXTREMELY slowly and is still ineffective (i.e. corrects Obama to drama)\n",
    "from nltk.corpus import words\n",
    "from textblob import Word\n",
    "\n",
    "correction_threshold = 0.95\n",
    "\n",
    "def correct(sentence):\n",
    "    sentence = replaceTwoOrMore(sentence)\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    corrected_tokens = []\n",
    "    for i in tokens:\n",
    "        if i in words.words():\n",
    "            corrected_tokens.append(i)\n",
    "        else:\n",
    "            w = Word(i)\n",
    "            if w.spellcheck()[0][1] >= correction_threshold:\n",
    "                corrected_tokens.append(w.spellcheck()[0][0])\n",
    "            else:\n",
    "                corrected_tokens.append(i)\n",
    "\n",
    "    clean_sentence = \" \".join(corrected_tokens)\n",
    "    return clean_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Processing - must manually select how to stem / lemmatize\n",
    "porter = nltk.PorterStemmer() #porter.stem\n",
    "snowball = nltk.SnowballStemmer('english') #snowball.stem\n",
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer() #wordnet_lemmatizer.lemmatize\n",
    "\n",
    "# Add additional stopwords\n",
    "add_stop = [\"'s\",\"n't\",\"''\",\"'m\",'http','.com',\"--\",\"gt\"]\n",
    "\n",
    "def NLTKprocess(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmed_list = []\n",
    "    for i in tokens:\n",
    "        if(i in stop or re.search(\"^[a-zA-Z0-9\\-']*$\", i) is None):\n",
    "            continue\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(i)\n",
    "            if lemma not in add_stop:\n",
    "                stemmed_list.append(lemma)\n",
    "    return stemmed_list\n",
    "\n",
    "# Ngram with no stop words or stemming\n",
    "def NLTKngram(text,n):\n",
    "    return list(ngrams(text.split(), n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature with cleaned comments\n",
    "clean_text = []\n",
    "\n",
    "for i in df['comment']:\n",
    "    clean_text.append(processText(i))\n",
    "\n",
    "df['new_comment'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize cleaned comments. ngram functionality has been turned off as is not currently being used\n",
    "tokens = []\n",
    "ngram = []\n",
    "nwords = 2\n",
    "\n",
    "for i in df['new_comment']:\n",
    "    tokens.append(NLTKprocess(i))\n",
    "    #ngram.append(NLTKngram(i,nwords))  \n",
    "\n",
    "df['tokens'] = tokens\n",
    "#df['ngram'] = ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove comments below minimum length\n",
    "minimum_len = 1\n",
    "\n",
    "df = df[df['tokens'].map(len) > minimum_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>new_comment</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>I just realised you are right and that im an i...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>ds0n5o3</td>\n",
       "      <td>t3_7mxfem</td>\n",
       "      <td>t1_ds0mznv</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>i just realised you are right and that im an i...</td>\n",
       "      <td>[realised, right, im, idiot, bc, forgot, im, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jphuffinstuff</td>\n",
       "      <td>No problem! It happens to the best of us!</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>ds0ncnp</td>\n",
       "      <td>t3_7mxfem</td>\n",
       "      <td>t1_ds0n5o3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>no problem! it happens to the best of us!</td>\n",
       "      <td>[problem, happens, best, u]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thrashgoat555</td>\n",
       "      <td>What missing information would that be.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>ds0nfc8</td>\n",
       "      <td>t3_7n88zr</td>\n",
       "      <td>t1_ds0ki36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>what missing information would that be.</td>\n",
       "      <td>[missing, information, would]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adventure_Inc</td>\n",
       "      <td>As someone who lives in Chicago, this pisses m...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>ds0nfj5</td>\n",
       "      <td>t3_7nbjsc</td>\n",
       "      <td>t3_7nbjsc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>as someone who lives in chicago, this pisses m...</td>\n",
       "      <td>[someone, life, chicago, piss]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Happy new year btw :)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>ds0ng9h</td>\n",
       "      <td>t3_7mxfem</td>\n",
       "      <td>t1_ds0ncnp</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>happy new year btw :)</td>\n",
       "      <td>[happy, new, year, btw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doomsdayprophecy</td>\n",
       "      <td>You're acting like a naive foreigner who's hea...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.514765e+09</td>\n",
       "      <td>ds0nhqi</td>\n",
       "      <td>t3_7n2nkh</td>\n",
       "      <td>t1_drzsa2c</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>you're acting like a naive foreigner who's hea...</td>\n",
       "      <td>['re, acting, like, naive, foreigner, heard, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doomsdayprophecy</td>\n",
       "      <td>Foreign agents... And domestic reactionaries. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.514766e+09</td>\n",
       "      <td>ds0o4su</td>\n",
       "      <td>t3_7n6dyr</td>\n",
       "      <td>t1_drzvvew</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>foreign agents... and domestic reactionaries. ...</td>\n",
       "      <td>[foreign, agent, domestic, reactionary, classi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the_undine</td>\n",
       "      <td>Very telling how they do this stuff but never ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.514767e+09</td>\n",
       "      <td>ds0p9h1</td>\n",
       "      <td>t3_7n88zr</td>\n",
       "      <td>t1_drzwdqu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>very telling how they do this stuff but never ...</td>\n",
       "      <td>[telling, stuff, never, resign]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>There’s that fucking blood and soil group agai...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.514768e+09</td>\n",
       "      <td>ds0puie</td>\n",
       "      <td>t3_7nbjsc</td>\n",
       "      <td>t3_7nbjsc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>there’s that fucking blood and soil group agai...</td>\n",
       "      <td>[fucking, blood, soil, group, put, flyer, town]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blinkysmurf</td>\n",
       "      <td>Yea, cause it's, like, the same thing and a di...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.514769e+09</td>\n",
       "      <td>ds0qim6</td>\n",
       "      <td>t3_7n88zr</td>\n",
       "      <td>t3_7n88zr</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>yea, cause it's, like, the same thing and a di...</td>\n",
       "      <td>[yea, cause, like, thing, direct, comparison, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Beemallard</td>\n",
       "      <td>So you're telling me that it has the worldwide...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.514769e+09</td>\n",
       "      <td>ds0qiuc</td>\n",
       "      <td>t3_7n88zr</td>\n",
       "      <td>t1_drzzxm8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>so you're telling me that it has the worldwide...</td>\n",
       "      <td>['re, telling, worldwide, police, stat, americ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>barinnoma</td>\n",
       "      <td>Firstly, thanks for this reply. Really awesome...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.514769e+09</td>\n",
       "      <td>ds0qv88</td>\n",
       "      <td>t3_7n4sxr</td>\n",
       "      <td>t1_ds0itxx</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>firstly, thanks for this reply. really awesome...</td>\n",
       "      <td>[firstly, thanks, reply, really, awesome, info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>happysmash27</td>\n",
       "      <td>You know you can use IRC on the most modern de...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.514770e+09</td>\n",
       "      <td>ds0r7gw</td>\n",
       "      <td>t3_7n4sxr</td>\n",
       "      <td>t1_ds07ceg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>you know you can use irc on the most modern de...</td>\n",
       "      <td>[know, use, irc, modern, device, right]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PM_ME_SAD_STUFF_PLZ</td>\n",
       "      <td>Aw so cute. You think the police are actually ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.514770e+09</td>\n",
       "      <td>ds0rgzh</td>\n",
       "      <td>t3_7n88zr</td>\n",
       "      <td>t1_ds0ewkd</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>aw so cute. you think the police are actually ...</td>\n",
       "      <td>[aw, cute, think, police, actually, protect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Decaf-Vampire</td>\n",
       "      <td>I didn't see her discounting revolutionary app...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.514770e+09</td>\n",
       "      <td>ds0rma9</td>\n",
       "      <td>t3_7n47l0</td>\n",
       "      <td>t1_drz4tt3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>i didn't see her discounting revolutionary app...</td>\n",
       "      <td>[see, discounting, revolutionary, approach, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kropatrick</td>\n",
       "      <td>1.) All the comrades in this sub\\r\\n2.) All th...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.514771e+09</td>\n",
       "      <td>ds0rn7a</td>\n",
       "      <td>t3_7n8jxf</td>\n",
       "      <td>t3_7n8jxf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.) all the comrades in this sub 2.) all the c...</td>\n",
       "      <td>[1, comrade, sub, 2, comrade, sub, 3, comrade,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Decaf-Vampire</td>\n",
       "      <td>The flippant and humorous manner in which Cont...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.514771e+09</td>\n",
       "      <td>ds0rslc</td>\n",
       "      <td>t3_7n47l0</td>\n",
       "      <td>t1_drzogo5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>the flippant and humorous manner in which cont...</td>\n",
       "      <td>[flippant, humorous, manner, contra, treated, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Rev1917-2017</td>\n",
       "      <td>No that is clearly the amount of Americans kil...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.514771e+09</td>\n",
       "      <td>ds0s90t</td>\n",
       "      <td>t3_7n88zr</td>\n",
       "      <td>t1_ds0qiuc</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>no that is clearly the amount of americans kil...</td>\n",
       "      <td>[clearly, amount, american, killed, like, clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Rev1917-2017</td>\n",
       "      <td>The cops are the suspicious ones. That's the j...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.514771e+09</td>\n",
       "      <td>ds0sa3x</td>\n",
       "      <td>t3_7n88zr</td>\n",
       "      <td>t1_ds029nx</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>the cops are the suspicious ones. that's the j...</td>\n",
       "      <td>[cop, suspicious, one, joke]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Rev1917-2017</td>\n",
       "      <td>What is possibly missing from this?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.514772e+09</td>\n",
       "      <td>ds0sccb</td>\n",
       "      <td>t3_7n88zr</td>\n",
       "      <td>t1_ds0ki36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>what is possibly missing from this?</td>\n",
       "      <td>[possibly, missing]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                            comment  \\\n",
       "0             [deleted]  I just realised you are right and that im an i...   \n",
       "1         jphuffinstuff          No problem! It happens to the best of us!   \n",
       "2         thrashgoat555            What missing information would that be.   \n",
       "3         Adventure_Inc  As someone who lives in Chicago, this pisses m...   \n",
       "4             [deleted]                             Happy new year btw :)    \n",
       "5      doomsdayprophecy  You're acting like a naive foreigner who's hea...   \n",
       "6      doomsdayprophecy  Foreign agents... And domestic reactionaries. ...   \n",
       "7            the_undine  Very telling how they do this stuff but never ...   \n",
       "8             [deleted]  There’s that fucking blood and soil group agai...   \n",
       "9           blinkysmurf  Yea, cause it's, like, the same thing and a di...   \n",
       "10           Beemallard  So you're telling me that it has the worldwide...   \n",
       "11            barinnoma  Firstly, thanks for this reply. Really awesome...   \n",
       "12         happysmash27  You know you can use IRC on the most modern de...   \n",
       "13  PM_ME_SAD_STUFF_PLZ  Aw so cute. You think the police are actually ...   \n",
       "14        Decaf-Vampire  I didn't see her discounting revolutionary app...   \n",
       "15           Kropatrick  1.) All the comrades in this sub\\r\\n2.) All th...   \n",
       "16        Decaf-Vampire  The flippant and humorous manner in which Cont...   \n",
       "17         Rev1917-2017  No that is clearly the amount of Americans kil...   \n",
       "18         Rev1917-2017  The cops are the suspicious ones. That's the j...   \n",
       "19         Rev1917-2017                What is possibly missing from this?   \n",
       "\n",
       "    score   created_utc       id    link_id   parent_id  hour  day  month  \\\n",
       "0     2.0  1.514765e+09  ds0n5o3  t3_7mxfem  t1_ds0mznv   0.0  1.0    1.0   \n",
       "1     1.0  1.514765e+09  ds0ncnp  t3_7mxfem  t1_ds0n5o3   0.0  1.0    1.0   \n",
       "2     3.0  1.514765e+09  ds0nfc8  t3_7n88zr  t1_ds0ki36   0.0  1.0    1.0   \n",
       "3    17.0  1.514765e+09  ds0nfj5  t3_7nbjsc   t3_7nbjsc   0.0  1.0    1.0   \n",
       "4     1.0  1.514765e+09  ds0ng9h  t3_7mxfem  t1_ds0ncnp   0.0  1.0    1.0   \n",
       "5    -2.0  1.514765e+09  ds0nhqi  t3_7n2nkh  t1_drzsa2c   0.0  1.0    1.0   \n",
       "6     0.0  1.514766e+09  ds0o4su  t3_7n6dyr  t1_drzvvew   0.0  1.0    1.0   \n",
       "7     0.0  1.514767e+09  ds0p9h1  t3_7n88zr  t1_drzwdqu   0.0  1.0    1.0   \n",
       "8    17.0  1.514768e+09  ds0puie  t3_7nbjsc   t3_7nbjsc   0.0  1.0    1.0   \n",
       "9     0.0  1.514769e+09  ds0qim6  t3_7n88zr   t3_7n88zr   1.0  1.0    1.0   \n",
       "10    1.0  1.514769e+09  ds0qiuc  t3_7n88zr  t1_drzzxm8   1.0  1.0    1.0   \n",
       "11    1.0  1.514769e+09  ds0qv88  t3_7n4sxr  t1_ds0itxx   1.0  1.0    1.0   \n",
       "12    1.0  1.514770e+09  ds0r7gw  t3_7n4sxr  t1_ds07ceg   1.0  1.0    1.0   \n",
       "13    6.0  1.514770e+09  ds0rgzh  t3_7n88zr  t1_ds0ewkd   1.0  1.0    1.0   \n",
       "14    2.0  1.514770e+09  ds0rma9  t3_7n47l0  t1_drz4tt3   1.0  1.0    1.0   \n",
       "15    7.0  1.514771e+09  ds0rn7a  t3_7n8jxf   t3_7n8jxf   1.0  1.0    1.0   \n",
       "16    4.0  1.514771e+09  ds0rslc  t3_7n47l0  t1_drzogo5   1.0  1.0    1.0   \n",
       "17    7.0  1.514771e+09  ds0s90t  t3_7n88zr  t1_ds0qiuc   1.0  1.0    1.0   \n",
       "18    3.0  1.514771e+09  ds0sa3x  t3_7n88zr  t1_ds029nx   1.0  1.0    1.0   \n",
       "19    3.0  1.514772e+09  ds0sccb  t3_7n88zr  t1_ds0ki36   1.0  1.0    1.0   \n",
       "\n",
       "      year                                        new_comment  \\\n",
       "0   2018.0  i just realised you are right and that im an i...   \n",
       "1   2018.0          no problem! it happens to the best of us!   \n",
       "2   2018.0            what missing information would that be.   \n",
       "3   2018.0  as someone who lives in chicago, this pisses m...   \n",
       "4   2018.0                             happy new year btw :)    \n",
       "5   2018.0  you're acting like a naive foreigner who's hea...   \n",
       "6   2018.0  foreign agents... and domestic reactionaries. ...   \n",
       "7   2018.0  very telling how they do this stuff but never ...   \n",
       "8   2018.0  there’s that fucking blood and soil group agai...   \n",
       "9   2018.0  yea, cause it's, like, the same thing and a di...   \n",
       "10  2018.0  so you're telling me that it has the worldwide...   \n",
       "11  2018.0  firstly, thanks for this reply. really awesome...   \n",
       "12  2018.0  you know you can use irc on the most modern de...   \n",
       "13  2018.0  aw so cute. you think the police are actually ...   \n",
       "14  2018.0  i didn't see her discounting revolutionary app...   \n",
       "15  2018.0  1.) all the comrades in this sub 2.) all the c...   \n",
       "16  2018.0  the flippant and humorous manner in which cont...   \n",
       "17  2018.0  no that is clearly the amount of americans kil...   \n",
       "18  2018.0  the cops are the suspicious ones. that's the j...   \n",
       "19  2018.0                what is possibly missing from this?   \n",
       "\n",
       "                                               tokens  \n",
       "0   [realised, right, im, idiot, bc, forgot, im, s...  \n",
       "1                         [problem, happens, best, u]  \n",
       "2                       [missing, information, would]  \n",
       "3                      [someone, life, chicago, piss]  \n",
       "4                             [happy, new, year, btw]  \n",
       "5   ['re, acting, like, naive, foreigner, heard, m...  \n",
       "6   [foreign, agent, domestic, reactionary, classi...  \n",
       "7                     [telling, stuff, never, resign]  \n",
       "8     [fucking, blood, soil, group, put, flyer, town]  \n",
       "9   [yea, cause, like, thing, direct, comparison, ...  \n",
       "10  ['re, telling, worldwide, police, stat, americ...  \n",
       "11  [firstly, thanks, reply, really, awesome, info...  \n",
       "12            [know, use, irc, modern, device, right]  \n",
       "13       [aw, cute, think, police, actually, protect]  \n",
       "14  [see, discounting, revolutionary, approach, th...  \n",
       "15  [1, comrade, sub, 2, comrade, sub, 3, comrade,...  \n",
       "16  [flippant, humorous, manner, contra, treated, ...  \n",
       "17  [clearly, amount, american, killed, like, clea...  \n",
       "18                       [cop, suspicious, one, joke]  \n",
       "19                                [possibly, missing]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people</td>\n",
       "      <td>3439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>2144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>would</td>\n",
       "      <td>1837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>think</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anarchist</td>\n",
       "      <td>1585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'re</td>\n",
       "      <td>1486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>one</td>\n",
       "      <td>1476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>get</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>state</td>\n",
       "      <td>1206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thing</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>even</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>make</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>way</td>\n",
       "      <td>1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>right</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>also</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  Frequency\n",
       "0      people       3439\n",
       "1        like       2144\n",
       "2       would       1837\n",
       "3       think       1613\n",
       "4   anarchist       1585\n",
       "5         're       1486\n",
       "6         one       1476\n",
       "7         get       1252\n",
       "8       state       1206\n",
       "9       thing       1185\n",
       "10       even       1089\n",
       "11       make       1047\n",
       "12        way       1010\n",
       "13      right        995\n",
       "14       also        993"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top words by occurence\n",
    "flatten = [item for sublist in df['tokens'] for item in sublist]\n",
    "word_dist = nltk.FreqDist(flatten)\n",
    "\n",
    "top_N = 15\n",
    "\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "rslt[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textblob sentiment approach\n",
    "sentiment_polarity = []\n",
    "sentiment_subjectivity = []\n",
    "\n",
    "for i in range(len(df['new_comment'])):\n",
    "    zen = TextBlob(df['new_comment'].iloc[i])\n",
    "    sentiment_polarity.append(zen.sentiment.polarity)\n",
    "    sentiment_subjectivity.append(zen.sentiment.subjectivity)\n",
    "    \n",
    "df['blob_polarity'] = np.around(sentiment_polarity,2)\n",
    "df['blob_subj'] = np.around(sentiment_subjectivity,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vader sentiment approach\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "vader_compound = []\n",
    "vader_neg = []\n",
    "vader_neu = []\n",
    "vader_pos = []\n",
    "\n",
    "for i in range(len(df['new_comment'])):\n",
    "    vader = analyser.polarity_scores(df['new_comment'][i])\n",
    "    vader_compound.append(vader['compound'])\n",
    "    vader_neg.append(vader['neg'])\n",
    "    vader_neu.append(vader['neu'])\n",
    "    vader_pos.append(vader['pos'])\n",
    "    \n",
    "df['v_compound'] = np.around(vader_compound,2)\n",
    "df['v_negative'] = np.around(vader_neg,2)\n",
    "df['v_neutral'] = np.around(vader_neu,2)\n",
    "df['v_positive'] = np.around(vader_pos,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textstat.textstat import textstat as ts\n",
    "\n",
    "flesch = [] #http://www.readabilityformulas.com/flesch-grade-level-readability-formula.php\n",
    "gunning_fog = [] #https://en.wikipedia.org/wiki/Gunning_fog_index\n",
    "avg_syllables = []\n",
    "difficult_words = []\n",
    "num_words = []\n",
    "\n",
    "for i in range(len(df['new_comment'])):\n",
    "    flesch.append(ts.flesch_kincaid_grade(df['new_comment'][i]))\n",
    "    gunning_fog.append(ts.gunning_fog(df['new_comment'][i]))\n",
    "    avg_syllables.append(ts.syllable_count(df['new_comment'][i]) / ts.lexicon_count(df['new_comment'][i]))\n",
    "    difficult_words.append(ts.difficult_words(df['new_comment'][i]) / ts.lexicon_count(df['new_comment'][i]))\n",
    "    num_words.append(ts.lexicon_count(df['new_comment'][i]))\n",
    "    \n",
    "# Create new columns\n",
    "df['flesch'] = np.around(flesch,2)\n",
    "df['gunning_fog'] = np.around(gunning_fog,2)\n",
    "df['avg_syllables'] = np.around(avg_syllables,2)\n",
    "df['difficult_words'] = np.around(difficult_words,2)\n",
    "df['word_count'] = np.around(num_words,2)\n",
    "\n",
    "# Remove negative values\n",
    "df['flesch'] = df['flesch'].clip(lower=0)\n",
    "df['gunning_fog'] = df['gunning_fog'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11083 entries, 0 to 11082\n",
      "Data columns (total 24 columns):\n",
      "author             11083 non-null object\n",
      "comment            11083 non-null object\n",
      "score              11083 non-null float64\n",
      "created_utc        11083 non-null float64\n",
      "id                 11083 non-null object\n",
      "link_id            11083 non-null object\n",
      "parent_id          11083 non-null object\n",
      "hour               11083 non-null float64\n",
      "day                11083 non-null float64\n",
      "month              11083 non-null float64\n",
      "year               11083 non-null float64\n",
      "new_comment        11083 non-null object\n",
      "tokens             11083 non-null object\n",
      "blob_polarity      11083 non-null float64\n",
      "blob_subj          11083 non-null float64\n",
      "v_compound         11083 non-null float64\n",
      "v_negative         11083 non-null float64\n",
      "v_neutral          11083 non-null float64\n",
      "v_positive         11083 non-null float64\n",
      "flesch             11083 non-null float64\n",
      "gunning_fog        11083 non-null float64\n",
      "avg_syllables      11083 non-null float64\n",
      "difficult_words    11083 non-null float64\n",
      "word_count         11083 non-null int32\n",
      "dtypes: float64(16), int32(1), object(7)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Output (Based on 11083 Comments in Top)\n",
      "\n",
      "Sentiment Analysis:\n",
      "Overall: 0.00\n",
      "Negative Polarity: 0.17\n",
      "Neutral Polarity: 0.77\n",
      "Positive Polarity: 0.18\n",
      "\n",
      "Writing Statistics:\n",
      "Flesch–Kincaid Grade: 8.0\n",
      "Avg Number of Syllables per Word: 1.53\n",
      "Proportion of Difficult Words: 0.22\n",
      "\n",
      "Most Common Words:\n",
      "        Word  Frequency\n",
      "0     people       3439\n",
      "1       like       2144\n",
      "2      would       1837\n",
      "3      think       1613\n",
      "4  anarchist       1585\n",
      "\n",
      "Note: All values represent averages\n"
     ]
    }
   ],
   "source": [
    "# Create summary report to ensure values make sense (no errors)\n",
    "print('Summary Output (Based on {} Comments in Top)'.format(len(df)))\n",
    "print()\n",
    "\n",
    "print(\"Sentiment Analysis:\")\n",
    "avg_sentiment = df['v_compound'].mean()\n",
    "# Polarity represents the average of the sentiment when it is non-zero\n",
    "avg_negative = df['v_negative'][df['v_negative']>0].mean()\n",
    "avg_neutral = df['v_neutral'][df['v_neutral']>0].mean()\n",
    "avg_positive = df['v_positive'][df['v_positive']>0].mean()\n",
    "                  \n",
    "print(\"Overall: {:.2f}\".format(avg_sentiment))\n",
    "print(\"Negative Polarity: {:.2f}\".format(avg_negative))\n",
    "print(\"Neutral Polarity: {:.2f}\".format(avg_neutral))\n",
    "print(\"Positive Polarity: {:.2f}\".format(avg_positive))\n",
    "print()\n",
    "\n",
    "print('Writing Statistics:')\n",
    "avg_flesch = df['flesch'].mean()\n",
    "avg_syllables = df['avg_syllables'].mean()\n",
    "avg_difficult_words = df['difficult_words'].mean()\n",
    "\n",
    "print(\"Flesch–Kincaid Grade: {:.1f}\".format(avg_flesch))\n",
    "print(\"Avg Number of Syllables per Word: {:.2f}\".format(avg_syllables))\n",
    "print(\"Proportion of Difficult Words: {:.2f}\".format(avg_difficult_words))\n",
    "print()\n",
    "\n",
    "print('Most Common Words:')\n",
    "print(rslt[:5])\n",
    "print()\n",
    "\n",
    "print(\"Note: All values represent averages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed comments to CSV\n",
    "comm_title = subreddit+'_processed_comments.csv'\n",
    "df.to_csv(comm_title,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
